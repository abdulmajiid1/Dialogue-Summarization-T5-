{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gl99sbG4qraF",
    "outputId": "e27b6c9a-3b53-4c94-c21a-a09a9040fd2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/44.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /simple/peft/\u001b[0m\u001b[33m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m137.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m380.9/380.9 kB\u001b[0m \u001b[31m37.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m59.1/59.1 MB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m556.4/556.4 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m512.3/512.3 kB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m18.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m120.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h‚úÖ Installation Complete.\n",
      "‚ö†Ô∏è IF YOU SEE ERRORS ABOVE: Go to 'Runtime > Restart session' and run this cell again.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 1: ENVIRONMENT SETUP (UPDATED)\n",
    "# ==========================================\n",
    "# We use -U to force the latest compatible versions of all libraries\n",
    "!pip install -q -U transformers accelerate bitsandbytes peft datasets evaluate rouge_score\n",
    "\n",
    "print(\"‚úÖ Installation Complete.\")\n",
    "print(\n",
    "    \"‚ö†Ô∏è IF YOU SEE ERRORS ABOVE: Go to 'Runtime > Restart session' and run this cell again.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35,
     "referenced_widgets": [
      "55bfc6480c7e451896d5344724b653b5",
      "0d110bdb1e954f03b1d8f1b99724a0a9",
      "e5e11868cd15412c97d115ec9f2f453e",
      "b0a4d1eabd2145dbb294e92b982e50cb",
      "cd3c58892184421a9715e394909a5bc1",
      "9dac2337c7564a8a9f975595ef1bfe0a",
      "3076972c25814faa99330e406d24f7bf",
      "6736166715564643ade07644c80c44a0",
      "6b02b0647dd148ee9e6d2a14bfd1964c",
      "214acab6325e460ab98cbe5b5c9ea497",
      "6457786c5d8f4a40b7f0945bc516380d",
      "cc8133e8ebd6461d969469d7fabdcf2a",
      "c95d417fb1704ae590b266daf917c9d9",
      "61fdf4199c2248f1ad69069d31a6769a",
      "040f42f93ad54d239b77fa93aaaf4a50",
      "4f0628174a194fc496d74f55bb6c5a44",
      "3f7b15c3c99945f3986cbb2cf7ecb05a",
      "696942c36d904f91a6e739db6f1791c8",
      "8a0fa79b5af74f6696a1e94072099c30",
      "9435fb767fe04819a086f1d8bdf22d48"
     ]
    },
    "id": "cCdv01pSbYc3",
    "outputId": "a804a9ec-6efb-4fe6-8dca-f195d1c5e08e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please paste your Hugging Face token in the box below:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55bfc6480c7e451896d5344724b653b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv‚Ä¶"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 2: HUGGING FACE AUTHENTICATION\n",
    "# ==========================================\n",
    "from huggingface_hub import login\n",
    "\n",
    "print(\"Please paste your Hugging Face token in the box below:\")\n",
    "login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 214,
     "referenced_widgets": [
      "7ddf31c227714ce7a9a1f9ab13a75e78",
      "4492f8b1c84442f2b9858f997e084823",
      "0a6da74fffa84ea3a949b5e2bd095ce8",
      "01f95a5caea44d3b8428b84f08c24b2c",
      "7d16744157a443dfbe987fbf0a2f6145",
      "57b6ff5f1a1e4b1e88130a1511c63c6a",
      "719b1fb70f7b44579ac319e0f59281ef",
      "447d4a06d6f049b085fe344268224282",
      "44779135aeb047828c25f0e89a08b338",
      "9ef3195bfb0e4e7c8e50f0a500122a89",
      "565e257ee91843b38edaa1bc98292bdc"
     ]
    },
    "id": "JwbKdckgwt24",
    "outputId": "307c32d5-2a22-4dab-ae28-6d2aa38cfc10"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/flan-t5-base with 4-bit quantization...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ddf31c227714ce7a9a1f9ab13a75e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded on cuda:0 with 4-bit weights.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 3: LOAD QUANTIZED MODEL\n",
    "# ==========================================\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig\n",
    "\n",
    "# 1. Define Model ID\n",
    "model_id = \"google/flan-t5-base\"\n",
    "\n",
    "# 2. Configure 4-bit NormalFloat (NF4) Quantization\n",
    "# This aligns with your proposal's goal of \"Resource Efficiency\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",  # The specific 4-bit format\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # Compute in half-precision (speed)\n",
    "    bnb_4bit_use_double_quant=True,  # Double quantization for memory savings\n",
    ")\n",
    "\n",
    "# 3. Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 4. Load Model\n",
    "print(f\"Loading {model_id} with 4-bit quantization...\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",  # Automatically assigns layers to the T4 GPU\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded on {model.device} with 4-bit weights.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYyVMBhRyBF7",
    "outputId": "16d28b4a-0623-4bc7-903a-c8a6da97e31c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,769,472 || all params: 249,347,328 || trainable%: 0.7096\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 4: LORA ADAPTER CONFIGURATION\n",
    "# ==========================================\n",
    "from peft import LoraConfig, get_peft_model, TaskType, prepare_model_for_kbit_training\n",
    "\n",
    "# 1. Prepare model for k-bit training (stabilizes the quantized weights)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# 2. Define LoRA Config\n",
    "# We target 'q' (Query) and 'v' (Value) attention layers as per standard practice\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank: Higher = more parameters, better learning\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    ")\n",
    "\n",
    "# 3. Inject LoRA Adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# 4. Verification\n",
    "model.print_trainable_parameters()\n",
    "# Expected output: \"trainable params: ~1.7M || all params: ~250M || trainable%: ~0.6%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409,
     "referenced_widgets": [
      "f74c9113444d49f8b316d619a4508422",
      "ae2dad27cc1e4723bc50aaf7871383b6",
      "f4d97109f7924d279231a5e48d1d2d74",
      "04a891ad7f17407e88d3e4693282c55a",
      "7443bcda596046fa934382a53d683867",
      "e7b901d7eb1c4b228e8be71045801b21",
      "04b72aab033c460b8a6684dff492e527",
      "42c0f6c13e294ac8a75300e27dbd8568",
      "7215d35114384158afcc6afc541a8295",
      "317b375582f646b8a401c15af20bd199",
      "e1419d8fc8e4449583f74165e88fc547",
      "88e158830ce0445fbebdd4fb69fbd176",
      "62c30702b1214eef98740cac23e87b7e",
      "1c70d6e497bf40b3bd26f5b4a1dceef6",
      "58e26782718247a18670bbfd7bc2bbdc",
      "56b85e66c07c4883a736d4ba9e41a16c",
      "12718bc72f4c4388a9f8553d37d2febb",
      "69d7a14e5f9f41bfa66094347deae5f7",
      "c37655468a6e4a9daf473678feb6c6d7",
      "b135b535695941dcb443227ce0c54c02",
      "f452fcff047b4eb28cd9afea491b5e19",
      "039e5194c2d94528a5a2435ff241eb9b",
      "cee0c1edf09c4f1288bb9c70f85f657d",
      "cb851ff4167649758243d3a9a180425f",
      "12755e60151a4d2ea4008d22a45f0ee5",
      "f30dbb7a92fe4990941fad5da1334b12",
      "bd6bcb81119b466dbedd6683e7b5b8e1",
      "dd4de374f83942d0bb88dac02f47e358",
      "fc2d04c77f2a4d2ca4a0f044a8d5feab",
      "075d7120dc76460394ecb35f3ad96d2e",
      "751957b8d0a14243833e96304f405b70",
      "5560bfa19d714a0b909f153d1957eadb",
      "380b31ffde6543668761ffab8ad081fe",
      "60596cb9d96642499d08581e26aaf553",
      "ed86e9b299dd4925b75902e362f81bac",
      "82a9640f25df48dfb7acc349322e3fef",
      "e1563fd8d32b41168741830b63928c7a",
      "74914cca77b5443bb23783c0db60fb7d",
      "032e8f8e62b24491901aa0f09c4347dd",
      "b246b58d8d3740cdb990fd00499bf9a0",
      "b16bae57c9994a4a912a4056321211a6",
      "9c78ae19c34941f1addc58084166bbe0",
      "29c300f6d1f6489fbc34a0ce06629104",
      "9fa29295cea44348bd91659df603a8a6",
      "d288bd51dd4442e190a05f691bac9b6e",
      "20b67ddcebad444e930b017adc4bdc9c",
      "7609f4a0b5fe4f94aa62d9f9d6b173c7",
      "1f18b13cf4694aadbfdb02ed2e9b9191",
      "b0a2c109ed5d4c36b509f8fd16ac0144",
      "133cf20abb3b4fe68e1d39e06fba072c",
      "6ba2e65b74974dc59fb26c649df5e11f",
      "efb99e8c08c548a5b13aa0449ce9336e",
      "043ca3c2922d4841901c97862a72148f",
      "6e8b58ac87a04ffda5556c5c645d1e4d",
      "06f8ee45ec56497ba7a79ac09c3207df",
      "c8be51879d234b5a92b86c5686425ee2",
      "06a52a8f7b1740e8b8b5e5566a763e60",
      "2e62b9d187dd480584dc94a0af22fa8e",
      "784d9a54380346119b1e8055345476c4",
      "f725284c6e5d4c0482f4da260a0de7fd",
      "0b46f7d481e942fcb4e85af57ece37fe",
      "541757ca57ed4c548baf309fef7eb486",
      "2259e8e0a6b1409aaeef59e88991d599",
      "da584a59e82f4405802d657ad35fd393",
      "6a71dcc2bf2448c09a39240b75a054ec",
      "50e0676536a4476ba25b9f9f3d8ba123",
      "0810af41a2e84d7ab2a17bdda27e4bf7",
      "d4b682d7489f4ac397f22b25621ddb66",
      "d8cc0c9d988f486e959756beaddf3e55",
      "32edae7f0b37419f84e1843e379e5c7e",
      "72b41da255694483bcd22d21222f8f19",
      "ac20db55b05f423bbfa9282fd917a7b4",
      "a03f7824acf145ec9d8d9da94331b2e6",
      "a005fb90f88440ac8cdc289daa523d8b",
      "909292d28d354921816d20cc5d71b5f1",
      "1490d6762f264df6b8c632767c8eb987",
      "8217af74d77f45089bd6e324771e02ca",
      "d36bf1b972474c40b4380adb2db0766f",
      "a4233eeb2ef6452d995d699a16855b99",
      "7e278a71286f4fecbd8255ae91f004b2",
      "de48271a0c8245768bce8b4583c07a07",
      "8f98936a30f249199678ee265dc5343a",
      "f7ce4d6955e54e9a887f0706c5980696",
      "a7eb875abd0d4241b575f3d14e0c6466",
      "c3bbb7cb00b5417cbce14a6c9cd58bde",
      "95e7680abd254cec8f7d1d6f8a54cd5e",
      "ad502fae4ca04387959a04907d9ef39a",
      "16a48bef304e47ecb749a965e746b358",
      "8cedbaa41aff425aa7ac53b55ef9bf4b",
      "d9c249619c8842089997c1bb470d24bd",
      "fbd17d46ae3f4a9a9de93182d75c5bb4",
      "587f66ce1c204567be65762631c1bc9b",
      "1f3a5f85eee64cd98a8d74c7edded7ce",
      "4517fc6189da471994e4bb6988107518",
      "1543b6fd56394983a250ffef7112ef03",
      "769d559e4c7a4348985511011bf0f034",
      "05cda0d5e629433caf769c0ba7650ea1",
      "c1d1f63b43b14120b6d8d8eef3934886",
      "dde0f3fec63b4acea34a7a342360ee9d",
      "44c9bd35d75d4e408085771a59d71929",
      "27ae94ad42a34fe992786c379deca6c5",
      "15ad545604b44dc6a9940b687e93f1d5",
      "a1ed64df0a5f4922acf06d84c252b46f",
      "f85d9ccdd79e460a9820d639b5bc0b0f",
      "106851c2f59f4fdb8863dcb00466eeac",
      "dc3de13bddbc40c4ad4b1f0eba28dc19",
      "3998291b31b34b378c8491051427a3c4",
      "a9a0faa0b60342aeb4fff7a8eba43966",
      "d686e8a26ce3447982cb451d15ac56ba",
      "e0ccb193f69e4ef087900cf2bbcfe9b2"
     ]
    },
    "id": "aaMUOovpyoyk",
    "outputId": "99b54573-db61-460f-854f-5b1a4ff305cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading SAMSum dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f74c9113444d49f8b316d619a4508422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88e158830ce0445fbebdd4fb69fbd176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cee0c1edf09c4f1288bb9c70f85f657d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60596cb9d96642499d08581e26aaf553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d288bd51dd4442e190a05f691bac9b6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8be51879d234b5a92b86c5686425ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0810af41a2e84d7ab2a17bdda27e4bf7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SAMSum loaded successfully from 'knkarthick/samsum'\n",
      "Tokenizing dataset (this may take 1-2 minutes)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d36bf1b972474c40b4380adb2db0766f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14731 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cedbaa41aff425aa7ac53b55ef9bf4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/818 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44c9bd35d75d4e408085771a59d71929",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data ready! Train size: 14731\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 5: DATA PREPARATION (CORRECTED)\n",
    "# ==========================================\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load SAMSum from the reliable mirror\n",
    "print(\"Loading SAMSum dataset...\")\n",
    "try:\n",
    "    # This mirror is currently the most stable version of SAMSum on the Hub\n",
    "    dataset = load_dataset(\"knkarthick/samsum\")\n",
    "    print(\"‚úÖ SAMSum loaded successfully from 'knkarthick/samsum'\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading mirror: {e}\")\n",
    "    # Fallback to DialogSum if SAMSum is completely inaccessible\n",
    "    print(\"Falling back to 'knkarthick/dialogsum' (similar dialogue data)...\")\n",
    "    dataset = load_dataset(\"knkarthick/dialogsum\")\n",
    "\n",
    "\n",
    "# 2. Preprocessing Function\n",
    "# We prefix inputs with \"Summarize: \" for Flan-T5\n",
    "def preprocess_function(sample):\n",
    "    # Prefix input\n",
    "    inputs = [\n",
    "        \"Summarize the following conversation:\\n\" + doc for doc in sample[\"dialogue\"]\n",
    "    ]\n",
    "\n",
    "    # Tokenize Input\n",
    "    model_inputs = tokenizer(\n",
    "        inputs, max_length=1024, padding=\"max_length\", truncation=True\n",
    "    )\n",
    "\n",
    "    # Tokenize Target (Summary)\n",
    "    labels = tokenizer(\n",
    "        text_target=sample[\"summary\"],\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "\n",
    "    # Replace padding token ID with -100 (ignored by loss function)\n",
    "    labels[\"input_ids\"] = [\n",
    "        [(l if l != tokenizer.pad_token_id else -100) for l in label]\n",
    "        for label in labels[\"input_ids\"]\n",
    "    ]\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# 3. Apply Processing\n",
    "print(\"Tokenizing dataset (this may take 1-2 minutes)...\")\n",
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)\n",
    "\n",
    "print(f\"‚úÖ Data ready! Train size: {len(tokenized_dataset['train'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 267,
     "referenced_widgets": [
      "5c6fd5fd7e254e5395cf8b1d8ee1de06",
      "e9ffc81900784e31bfecc39d45fde6c6",
      "d15f3d1fb8fb43009d50ea4485451224",
      "4edfbc122bac446caa0439a0b7e08581",
      "294deb4790524cbb92c28a192dcc583d",
      "77a382c71e294f4884ccf21e6e4c7325",
      "e28fb50c0d574baabeb6018ae267cf52",
      "d1e02403de914701a73050f2ccef70a5",
      "75250342e1ef4cc7858da7f29748597b",
      "cb7b7e2df7f2450a844a596517cdcf17",
      "7ae004ccc8c14940951cca23b0799243"
     ]
    },
    "id": "CCKrHATDy8Lo",
    "outputId": "5aec0da1-af56-4ac7-b7af-a573016cb16a"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6fd5fd7e254e5395cf8b1d8ee1de06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1842' max='1842' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1842/1842 1:24:33, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Rouge1</th>\n",
       "      <th>Rouge2</th>\n",
       "      <th>Rougel</th>\n",
       "      <th>Rougelsum</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.460500</td>\n",
       "      <td>1.404691</td>\n",
       "      <td>47.142100</td>\n",
       "      <td>23.195600</td>\n",
       "      <td>39.548800</td>\n",
       "      <td>39.509200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1842, training_loss=1.4878273010253906, metrics={'train_runtime': 5079.3354, 'train_samples_per_second': 2.9, 'train_steps_per_second': 0.363, 'total_flos': 2.0334471187267584e+16, 'train_loss': 1.4878273010253906, 'epoch': 1.0})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 6: TRAINING EXECUTION\n",
    "# ==========================================\n",
    "from transformers import (\n",
    "    Seq2SeqTrainer,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    DataCollatorForSeq2Seq,\n",
    ")\n",
    "import evaluate\n",
    "\n",
    "# 1. Metrics (ROUGE)\n",
    "metric = evaluate.load(\"rouge\")\n",
    "\n",
    "\n",
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "\n",
    "    # Replace -100 in labels\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE\n",
    "    result = metric.compute(\n",
    "        predictions=decoded_preds, references=decoded_labels, use_stemmer=True\n",
    "    )\n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "\n",
    "# 2. Trainer Configuration\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"flan-t5-samsum-lora-final\",\n",
    "    per_device_train_batch_size=8,  # Standard for T4 GPU\n",
    "    per_device_eval_batch_size=8,\n",
    "    learning_rate=1e-3,  # High LR is good for LoRA\n",
    "    num_train_epochs=1,  # 1 Epoch is usually sufficient for this task\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    predict_with_generate=True,  # Generate summaries during eval\n",
    "    report_to=\"tensorboard\",\n",
    ")\n",
    "\n",
    "# 3. Initialize Trainer\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"test\"],\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 4. Start Training\n",
    "print(\"üöÄ Starting Training...\")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vVp3b3ynHZiJ",
    "outputId": "318021a1-0c67-4b83-d379-41147fb96a8b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Saving model to /content/drive/My Drive/FinalProject_FlanT5_Samsum...\n",
      "‚úÖ Model saved successfully!\n",
      "You can check your Google Drive folder to see the files.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 7: SAVE TO GOOGLE DRIVE\n",
    "# ==========================================\n",
    "from google.colab import drive\n",
    "\n",
    "# 1. Mount Drive (Pop-up will ask for permission)\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# 2. Define the save path\n",
    "save_path = \"/content/drive/My Drive/FinalProject_FlanT5_Samsum\"\n",
    "print(f\"Saving model to {save_path}...\")\n",
    "\n",
    "# 3. Save the adapter and the tokenizer\n",
    "trainer.model.save_pretrained(save_path)\n",
    "tokenizer.save_pretrained(save_path)\n",
    "\n",
    "print(\"‚úÖ Model saved successfully!\")\n",
    "print(f\"You can check your Google Drive folder to see the files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9rs1boIBIABc",
    "outputId": "aaf7747f-d34a-458e-ffce-e47d0a4b13e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "DIALOGUE:\n",
      "\n",
      "Joey: Hey, are we still on for the movies tonight?\n",
      "Chandler: Yeah, but I might be a little late. Work is crazy.\n",
      "Joey: No worries. Ross said he's coming too.\n",
      "Chandler: Oh great. Is he bringing meaningful conversation or dinosaurs?\n",
      "Joey: Dinosaurs, obviously.\n",
      "Chandler: Fine. I'll meet you guys at 8 PM in front of the theater.\n",
      "\n",
      "------------------------------\n",
      "MODEL SUMMARY:\n",
      "Chandler will meet Joey and Ross at 8 PM in front of the theater to see Dinosaurs.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 8: TEST THE MODEL (INFERENCE) - FIXED\n",
    "# ==========================================\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "# 1. Create a sample conversation\n",
    "sample_dialogue = \"\"\"\n",
    "Joey: Hey, are we still on for the movies tonight?\n",
    "Chandler: Yeah, but I might be a little late. Work is crazy.\n",
    "Joey: No worries. Ross said he's coming too.\n",
    "Chandler: Oh great. Is he bringing meaningful conversation or dinosaurs?\n",
    "Joey: Dinosaurs, obviously.\n",
    "Chandler: Fine. I'll meet you guys at 8 PM in front of the theater.\n",
    "\"\"\"\n",
    "\n",
    "# 2. Format inputs and move to GPU\n",
    "input_text = \"Summarize the following conversation:\\n\" + sample_dialogue\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 3. Generate Summary\n",
    "# CHANGE IS HERE: We added 'input_ids=' to make it a keyword argument\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs.input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=100, num_beams=5),\n",
    ")\n",
    "\n",
    "text_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"DIALOGUE:\\n{sample_dialogue}\")\n",
    "print(\"-\" * 30)\n",
    "print(f\"MODEL SUMMARY:\\n{text_output}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pJfXto5eOFV9",
    "outputId": "b8b3f0e1-5484-44d0-b981-d27bc413628d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bert_score\n",
      "  Downloading bert_score-0.3.13-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from bert_score) (2.9.0+cu126)\n",
      "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from bert_score) (2.2.2)\n",
      "Requirement already satisfied: transformers>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from bert_score) (4.57.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bert_score) (2.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from bert_score) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.31.1 in /usr/local/lib/python3.12/dist-packages (from bert_score) (4.67.1)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from bert_score) (3.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from bert_score) (25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert_score) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert_score) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.1->bert_score) (2025.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (3.20.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (4.15.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (75.2.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (3.1.6)\n",
      "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (12.6.80)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (12.6.4.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (11.3.0.4)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (10.3.7.77)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (11.7.1.2)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (12.5.4.2)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (2.27.5)\n",
      "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (3.3.20)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (12.6.77)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (12.6.85)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (1.11.1.6)\n",
      "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.0.0->bert_score) (3.5.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert_score) (0.36.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert_score) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert_score) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert_score) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers>=3.0.0->bert_score) (0.7.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (1.3.3)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (4.61.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (1.4.9)\n",
      "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->bert_score) (3.2.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->bert_score) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->bert_score) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->bert_score) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->bert_score) (2025.11.12)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers>=3.0.0->bert_score) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.1->bert_score) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.0.0->bert_score) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.0.0->bert_score) (3.0.3)\n",
      "Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: bert_score\n",
      "Successfully installed bert_score-0.3.13\n"
     ]
    }
   ],
   "source": [
    "# Install the missing library\n",
    "!pip install bert_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374,
     "referenced_widgets": [
      "e6942f5a165e43a9b832e0f4f30afaba",
      "a14fd1e235f84614b3eb4b6b2c3c5ea1",
      "b135b743376f4145af93eedb8aa7655b",
      "acd9ffd6f510421abd6822b5db960eef",
      "090a9b231e8a462ea070bc1d518ab14c",
      "35f2f9ecc49c4842af36fd8b56a15956",
      "e61b607117f948329ea786f34e430d84",
      "199f60b64c91408785b7e3067373ac1a",
      "0d2f49a39a6146d7ab57e9ee1a45f591",
      "286e08f89ff345c08a7c459188bd34cc",
      "24b79b98b6714edba73b447d7648ab44",
      "c75319f4b4ec445ebd325959363d1e12",
      "ffdef1509b94441bb9607e7458e9887a",
      "3648905fe4ad450e9a6323c302fffa57",
      "3d43ec66bf474fcf88b70cfddf94909b",
      "e25f2c09a9ed43af8e6c7226474a1427",
      "0ac3d6d3072a482fb07b7b1d49cdc053",
      "8c81fadae24a4d5e9e1b4372460d154d",
      "c355036deca34cb9b7d70ac9850cbc8f",
      "12d9dc6246a84613931b9a1518735888",
      "947ffc8797954d04b662019a1fb7bea2",
      "0905b11af5624a69989aa05bcbe8930e",
      "46f256b126d74788a20f16b59430718d",
      "5b1ea7ffb123499daeaff4b6cb553dea",
      "7db7d492180b4d43ad3d39faf05f177a",
      "b5ff86f64f934dc2bff9e436fe062825",
      "39b9662372774d9a86b26da7fab3eeb4",
      "88108ec09606440d86ee955b156146da",
      "91e74e70ef224f1680718df13018e95a",
      "8b806a5a3126427396bf5fd1c85f016d",
      "e5c9b0e3bf794a83876bb0efb8c83198",
      "60eb9014ee5b47f6bd44cd5e68296fdc",
      "2912c29b5cba4e319d00338d895e4b93",
      "c5bcc1a1887847ad870f39cf0588ebb9",
      "6afa67b19bf34ba5b56855134d7d53b4",
      "e7a4236b68934a5fac53edf119efba02",
      "3cf4004cd66148238f7e6883181a46e3",
      "bcf280d1fc1a4c64821ce358950c86d9",
      "97b59b0d3176433d8fd0bd350ec31e91",
      "047afef6fb4e4a4f903ae15b0e5dcc0e",
      "c78aaf32e7954797b1923fdaba3a8a34",
      "77ee7b0666b448ecbfe15b69c847e683",
      "b62dfba6dd1e445cb442a175c2a6423e",
      "ad9773a3d4c34cc4a4fb5b6b1b905f17",
      "824ce1d0a5134930a3b83f1e291e8c34",
      "9c8bc40450994ec8b07209d04a4d97ec",
      "3e72f5f2437a45b6b8292162126ed694",
      "39e2863039934fddad48c5528b310597",
      "ade3848e5a2c46b48db3975e7d52cc95",
      "61f166fa724746f98f44b0530d581512",
      "15440dfbde004a9c898cc40334d22cf9",
      "7e2b7e5d968d47f6a82966817e0ed206",
      "8fd14eca9a3743229ae9bd270fe8dea2",
      "24b1e26cdf5c4f909744bb68c805f721",
      "936abb189f9e480199db6b298bdf1db9",
      "4d4c4159ff9c4b08a201c6351c242a8d",
      "e31bda3cce4c4bd1a4c38cde5e2747a9",
      "fc4f825365ac4fc2a79d4d45d3a94344",
      "952db1079d004f63b4fb581a37e9fc56",
      "20fd4378cee14498b043e06fcd920920",
      "7e0a0a245efc413e9fe2d726643a85cf",
      "d7e078e07d8a465aaaea58b46b7a3b0b",
      "4aa8d81099ef43a7888d33f6924c0407",
      "a842f8fa66f34fa7b09ada2e549ea4a4",
      "fbcd42d6fb574296a52a0216dc500466",
      "02668126e1904e9684d0238c3fce99e5"
     ]
    },
    "id": "k7EnrppBPqQN",
    "outputId": "c4e56d5b-d761-4da9-9357-6083841e3a51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Loading BERTScore metric...\n",
      "Generating summaries for scoring (this takes ~1-2 mins)...\n",
      "Computing BERTScore...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6942f5a165e43a9b832e0f4f30afaba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c75319f4b4ec445ebd325959363d1e12",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f256b126d74788a20f16b59430718d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5bcc1a1887847ad870f39cf0588ebb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "824ce1d0a5134930a3b83f1e291e8c34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d4c4159ff9c4b08a201c6351c242a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "‚úÖ BERTScore (F1): 90.9513\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# CELL 9 (FIXED): GET BERTSCORE (NO TRAINING)\n",
    "# ==========================================\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import GenerationConfig\n",
    "\n",
    "# 1. Install Library (Just in case)\n",
    "!pip install -q bert_score\n",
    "\n",
    "# 2. Load Metric\n",
    "print(\"‚è≥ Loading BERTScore metric...\")\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "\n",
    "# 3. Generate Summaries for a slice of the test set\n",
    "# We take 50 examples to be fast\n",
    "print(\"Generating summaries for scoring (this takes ~1-2 mins)...\")\n",
    "test_slice = tokenized_dataset[\"test\"].select(range(50))\n",
    "input_ids = torch.tensor(test_slice[\"input_ids\"]).to(\"cuda\")\n",
    "\n",
    "# --- FIX IS HERE ---\n",
    "# We must use \"input_ids=\" explicitly\n",
    "summary_ids = model.generate(\n",
    "    input_ids=input_ids,  # <--- This was the missing part\n",
    "    generation_config=GenerationConfig(max_new_tokens=100, num_beams=2),\n",
    ")\n",
    "# -------------------\n",
    "\n",
    "decoded_preds = tokenizer.batch_decode(summary_ids, skip_special_tokens=True)\n",
    "\n",
    "# 4. Get References (Human Summaries)\n",
    "# Replace -100 with padding to decode\n",
    "labels = np.where(\n",
    "    np.array(test_slice[\"labels\"]) != -100, test_slice[\"labels\"], tokenizer.pad_token_id\n",
    ")\n",
    "decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "# 5. Compute the Score\n",
    "print(\"Computing BERTScore...\")\n",
    "bert_result = bertscore.compute(\n",
    "    predictions=decoded_preds, references=decoded_labels, lang=\"en\"\n",
    ")\n",
    "final_bertscore = np.mean(bert_result[\"f1\"]) * 100\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"‚úÖ BERTScore (F1): {final_bertscore:.4f}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 890,
     "referenced_widgets": [
      "588e56f257fd4f2ca5994d0924d97004",
      "7e61b5c77964498094fc55d53a4477f5",
      "1eac2048532342f1b1d8d90f26336994",
      "277035aec2924814b22af03eae391981",
      "3226e3777057460c9772ef9fdcd2f4a7",
      "ac3360ee83eb473488cbe168d97c96ff",
      "c421be3e9c9b47b79d3f685f073c5aa0",
      "a6cd0b9fe37e4640b1691755098c3a9f",
      "b674b9b80cb1484eb39421998985f816",
      "db5055ce091241f59fc2a47a7a571e2e",
      "818e68d946fb43f7bf9a36438bf3321a",
      "0a50bde560f34e78b172fed84cc084ab",
      "a3dbef7c5c9e4c8e91bca14f8bf7509b",
      "ffeb0ad9755148bc95a923c3482c166b",
      "dcb87273d30c440c899688a47554e452",
      "55fc42926ee04552abf9b570bcc7325d",
      "eb94a475b8a141a993eb62001ea6c61d",
      "439e1cfbc8754d3dbe1b8887579eccf7",
      "fff9329e44374232bf0f265374299bca",
      "022def24570d443ba74e6ab573ba77d0",
      "f44b8df6fb39447cac3ba9af30de9243",
      "c6c3aab514814a17a20af40342f5d7df",
      "d0b01e874fa34c4985e5a45eeb0cd9bf",
      "151dc06ec6bd48b3a3dbba10eeced1b1",
      "a85395702755443d88f6551d8ef23923",
      "26f8830ea8504635826ff80d2fd0d8be",
      "9861c1bd23c74657804a370a46659f6a",
      "735fa848db264005abe8887a86b38c36",
      "ed3596127e8e404fa05ab01005ff4d62",
      "639e9878af6a4debbeb0d9361e9490f1",
      "db2bb076293e4c41b9e5f5af818722aa",
      "6534ac9cd2284c55bbc4fb06879f6dd9",
      "f6a95097801a4155ac9ddcaf580eee19"
     ]
    },
    "id": "3V5jmLKCIyUF",
    "outputId": "ca1fc289-e8cc-485b-8433-5735445a6d5c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Loading your fine-tuned model from Drive... Please wait.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "588e56f257fd4f2ca5994d0924d97004",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a50bde560f34e78b172fed84cc084ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0b01e874fa34c4985e5a45eeb0cd9bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "Launching Demo...\n",
      "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
      "* Running on public URL: https://27e26f89998466759d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://27e26f89998466759d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# FINAL PROJECT DEMO: INTERACTIVE APP\n",
    "# ==========================================\n",
    "\n",
    "# 1. Install necessary libraries\n",
    "# We need Gradio for the interface and Transformers for the model\n",
    "!pip install -q gradio transformers peft torch\n",
    "\n",
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "from peft import PeftModel, PeftConfig\n",
    "from google.colab import drive\n",
    "\n",
    "# 2. Mount Drive to access your trained model\n",
    "# (If you are already mounted, this will just say \"Drive already mounted\")\n",
    "drive.mount(\"/content/drive\")\n",
    "\n",
    "# 3. Load the Model (The \"Baked Cake\")\n",
    "print(\"Loading your fine-tuned model from Drive... Please wait.\")\n",
    "\n",
    "# Path where we saved it earlier\n",
    "save_path = \"/content/drive/My Drive/FinalProject_FlanT5_Samsum\"\n",
    "\n",
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(save_path)\n",
    "\n",
    "# Load Base Model (Google Flan-T5)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    \"google/flan-t5-base\", device_map=\"auto\" if torch.cuda.is_available() else \"cpu\"\n",
    ")\n",
    "\n",
    "# Load Your Learned Adapters (LoRA)\n",
    "model = PeftModel.from_pretrained(base_model, save_path)\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "\n",
    "\n",
    "# 4. Define the Summarization Function\n",
    "def summarize_dialogue(input_text):\n",
    "    # Add the prefix we used during training\n",
    "    prompt = \"Summarize the following conversation:\\n\" + input_text\n",
    "\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    # Move to GPU if available\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    # Generate Summary\n",
    "    # We use beam search (num_beams=5) for high-quality text\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs.input_ids,\n",
    "            generation_config=GenerationConfig(\n",
    "                max_new_tokens=150,\n",
    "                num_beams=5,\n",
    "                repetition_penalty=2.5,  # Prevents repeating phrases\n",
    "                early_stopping=True,\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    # Decode result\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "\n",
    "# 5. Create the Web Interface\n",
    "# This builds the visual \"App\" your professor will see\n",
    "demo = gr.Interface(\n",
    "    fn=summarize_dialogue,\n",
    "    inputs=gr.Textbox(\n",
    "        lines=10,\n",
    "        placeholder=\"Paste a dialogue here (e.g., WhatsApp chat)...\",\n",
    "        label=\"Input Dialogue\",\n",
    "    ),\n",
    "    outputs=gr.Textbox(label=\"Generated Summary\"),\n",
    "    title=\"Dialogue Summarization System\",\n",
    "    description=\"Final Project Demo: Fine-Tuned Flan-T5 with LoRA Adapters.\",\n",
    "    examples=[\n",
    "        # Example 1\n",
    "        [\n",
    "            \"\"\"Joey: Hey, are we still on for the movies tonight?\n",
    "Chandler: Yeah, but I might be a little late. Work is crazy.\n",
    "Joey: No worries. Ross said he's coming too.\n",
    "Chandler: Oh great. Is he bringing meaningful conversation or dinosaurs?\n",
    "Joey: Dinosaurs, obviously.\n",
    "Chandler: Fine. I'll meet you guys at 8 PM.\"\"\"\n",
    "        ],\n",
    "        # Example 2\n",
    "        [\n",
    "            \"\"\"Customer: Hi, I bought a laptop yesterday but it won't turn on.\n",
    "Support: I'm sorry to hear that. Is the charging light on when you plug it in?\n",
    "Customer: No, nothing happens.\n",
    "Support: It sounds like a defective battery. Please bring it to the store for a replacement.\n",
    "Customer: Okay, I will come tomorrow morning. Thanks.\"\"\"\n",
    "        ],\n",
    "    ],\n",
    ")\n",
    "\n",
    "# 6. Launch the App\n",
    "print(\"Launching Demo...\")\n",
    "demo.launch(share=True, debug=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
