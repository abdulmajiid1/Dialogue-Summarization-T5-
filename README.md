# Dialogue Summarization with Flan-T5 & QLoRA

## ðŸ“Œ Project Overview
This project focuses on **Abstractive Dialogue Summarization** using a parameter-efficient fine-tuning (PEFT) approach. We fine-tuned the `google/flan-t5-base` model on the **SAMSum corpus** using **QLoRA** (Quantized Low-Rank Adaptation) to achieve high performance with limited computational resources.



## ðŸ“‚ Repository Structure
This project follows a professional modular architecture:

```text
Dialogue-Summarization/
â”œâ”€â”€ app/
â”‚   â””â”€â”€ app.py                # Interactive Gradio web application (Demo)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py             # Global hyperparameters and file paths
â”‚   â”œâ”€â”€ model_loader.py       # 4-bit Quantization (NF4) & LoRA configuration
â”‚   â””â”€â”€ train.py              # Main training script (Hugging Face Trainer)
â”œâ”€â”€ Final_Project_Fixed.ipynb # Full Project Notebook (Data Analysis, Training, Evaluation)
â”œâ”€â”€ requirements.txt          # List of dependencies
â””â”€â”€ README.md                 # Project Documentation


## ðŸš€ Technical Implementation
* **Base Model:** Flan-T5 Base (248M Parameters)
* **Optimization:**
    * **BitsAndBytes:** 4-bit Normal Float (NF4) quantization.
    * **LoRA:** Rank=16, Alpha=32, targeting Query/Value modules.
* **Dataset:** SAMSum (14k training dialogues).

## ðŸ“Š Results
The model was evaluated on the test set with the following metrics:
* **ROUGE-1:** 47.14
* **BERTScore (F1):** 90.95
* **Validation Loss:** 1.40 (No overfitting observed)

## ðŸ’» How to Run

### 1. Installation
Clone the repository and install the required dependencies:
```bash
pip install -r requirements.txt
